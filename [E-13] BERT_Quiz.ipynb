{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59d590a",
   "metadata": {},
   "source": [
    "## 필요한 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69fff6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4136a12",
   "metadata": {},
   "source": [
    "### GPU 메모리 오류\n",
    "InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n",
    "\n",
    "BERT 레이어를 전부 선언하고 위에 메모리 오류가 떳다. 찾아보니 아래와 같은 해결 방법이 있었다.\n",
    "\n",
    "[메모리 해결](https://inpages.tistory.com/155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0889757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79441dd4",
   "metadata": {},
   "source": [
    "- path 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd154c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_qna/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_qna/models'\n",
    "\n",
    "train_json_path = data_dir + '/KorQuAD_v1.0_train.json'\n",
    "dev_json_path = data_dir + '/KorQuAD_v1.0_dev.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38270526",
   "metadata": {},
   "source": [
    "- 파일 잘 불러 왔는지 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de5198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json_tree(data, indent=\"\"):\n",
    "    for key, value in data.items():\n",
    "        if type(value) == list:     # list 형태의 item은 첫번째 item만 출력\n",
    "            print(f'{indent}- {key}: [{len(value)}]')\n",
    "            print_json_tree(value[0], indent + \"  \")\n",
    "        else:\n",
    "            print(f'{indent}- {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3627f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- version: KorQuAD_v1.0_train\n",
      "- data: [1420]\n",
      "  - paragraphs: [3]\n",
      "    - qas: [8]\n",
      "      - answers: [1]\n",
      "        - text: 교향곡\n",
      "        - answer_start: 54\n",
      "      - id: 6566495-0-0\n",
      "      - question: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "    - context: 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "  - title: 파우스트_서곡\n",
      "- version: KorQuAD_v1.0_dev\n",
      "- data: [140]\n",
      "  - paragraphs: [2]\n",
      "    - qas: [7]\n",
      "      - answers: [1]\n",
      "        - text: 1989년 2월 15일\n",
      "        - answer_start: 0\n",
      "      - id: 6548850-0-0\n",
      "      - question: 임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?\n",
      "    - context: 1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의(폭력행위등처벌에관한법률위반)으로 지명수배되었다. 1989년 3월 12일 서울지방검찰청 공안부는 임종석의 사전구속영장을 발부받았다. 같은 해 6월 30일 평양축전에 임수경을 대표로 파견하여 국가보안법위반 혐의가 추가되었다. 경찰은 12월 18일~20일 사이 서울 경희대학교에서 임종석이 성명 발표를 추진하고 있다는 첩보를 입수했고, 12월 18일 오전 7시 40분 경 가스총과 전자봉으로 무장한 특공조 및 대공과 직원 12명 등 22명의 사복 경찰을 승용차 8대에 나누어 경희대학교에 투입했다. 1989년 12월 18일 오전 8시 15분 경 서울청량리경찰서는 호위 학생 5명과 함께 경희대학교 학생회관 건물 계단을 내려오는 임종석을 발견, 검거해 구속을 집행했다. 임종석은 청량리경찰서에서 약 1시간 동안 조사를 받은 뒤 오전 9시 50분 경 서울 장안동의 서울지방경찰청 공안분실로 인계되었다.\n",
      "  - title: 임종석\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터\n",
    "with open(train_json_path) as f:\n",
    "    train_json = json.load(f)\n",
    "    print_json_tree(train_json)\n",
    "\n",
    "# 검증 데이터\n",
    "with open(dev_json_path) as f:\n",
    "    dev_json = json.load(f)\n",
    "    print_json_tree(dev_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609d105",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889fc63",
   "metadata": {},
   "source": [
    "### (1) 띄어쓰기 단위 정보 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d826514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 판단하는 함수 \n",
    "def _is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _tokenize_whitespace(string):\n",
    "    word_tokens = []\n",
    "    char_to_word = []\n",
    "    prev_is_whitespace = True\n",
    "\n",
    "    for c in string:\n",
    "        if _is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "        else:\n",
    "            if prev_is_whitespace:\n",
    "                word_tokens.append(c)\n",
    "            else:\n",
    "                word_tokens[-1] += c\n",
    "            prev_is_whitespace = False    \n",
    "        char_to_word.append(len(word_tokens) - 1)\n",
    "    \n",
    "    return word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d466a895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' : 0\n",
      "'8' : 0\n",
      "'3' : 0\n",
      "'9' : 0\n",
      "'년' : 0\n",
      "' ' : 0\n",
      "'파' : 1\n",
      "'우' : 1\n",
      "'스' : 1\n",
      "'트' : 1\n",
      "'를' : 1\n",
      "' ' : 1\n",
      "'읽' : 2\n",
      "'었' : 2\n",
      "'다' : 2\n",
      "'.' : 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['1839년', '파우스트를', '읽었다.'], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1 = '1839년 파우스트를 읽었다.'\n",
    "\n",
    "# 첫번째 문장(string1)에 대해 띄어쓰기 영역 정보를 표시\n",
    "word_tokens, char_to_word = _tokenize_whitespace(string1)\n",
    "for c, i in zip(list(string1), char_to_word):\n",
    "    print(f'\\'{c}\\' : {i}')\n",
    "\n",
    "word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b365359",
   "metadata": {},
   "source": [
    "### (2) Tokenize by Vocab\n",
    "- BERT에서는 WordPiece 모델을 사용하지만, 이번 과제에서는 SentencePiece를 이용해 subword segmentation을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c94c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")\n",
    "\n",
    "def _tokenize_vocab(vocab, context_words):\n",
    "    word_to_token = []\n",
    "    context_tokens = []\n",
    "    for word in context_words:\n",
    "        word_to_token.append(len(context_tokens))\n",
    "        tokens = vocab.encode_as_pieces(word)\n",
    "        for token in tokens:\n",
    "            context_tokens.append(token)\n",
    "    return context_tokens, word_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25df9c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1839년', '파우스트를', '읽었다.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['▁1839', '년', '▁', '파우스트', '를', '▁읽', '었다', '.'], [0, 2, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_tokens)  # 처리해야 할 word 단위 입력\n",
    "\n",
    "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
    "context_tokens, word_to_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399fa47",
   "metadata": {},
   "source": [
    "### (3) imporve span\n",
    "- 데이터셋에서 context, question, answer을 찾아내기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3b74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_tokens에서 char_answer의 위치를 찾아 리턴하는 함수\n",
    "def _improve_span(vocab, context_tokens, token_start, token_end, char_answer):\n",
    "    token_answer = \" \".join(vocab.encode_as_pieces(char_answer))\n",
    "    for new_start in range(token_start, token_end + 1):\n",
    "        for new_end in range(token_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
    "            if text_span == token_answer:\n",
    "                return (new_start, new_end)\n",
    "    return (token_start, token_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c3a85c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_start: 54  token_end: 56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'교향곡'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = train_json['data'][0]['paragraphs'][0]['context']\n",
    "question = train_json['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "answer_text = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n",
    "answer_start = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "token_start, token_end = _improve_span(vocab, context, answer_start, answer_end, answer_text)\n",
    "print('token_start:', token_start, ' token_end:', token_end)\n",
    "context[token_start:token_end + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b6f37",
   "metadata": {},
   "source": [
    "### (4) 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b51327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_korquad(vocab, json_data, out_file):\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for data in tqdm(json_data[\"data\"]):\n",
    "            title = data[\"title\"]\n",
    "            for paragraph in data[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]\n",
    "                context_words, char_to_word = _tokenize_whitespace(context)\n",
    "\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    assert len(qa[\"answers\"]) == 1\n",
    "                    qa_id = qa[\"id\"]\n",
    "                    question = qa[\"question\"]\n",
    "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                    answer_start = qa[\"answers\"][0][\"answer_start\"]\n",
    "                    answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "                    assert answer_text == context[answer_start:answer_end + 1]\n",
    "\n",
    "                    word_start = char_to_word[answer_start]\n",
    "                    word_end = char_to_word[answer_end]\n",
    "\n",
    "                    word_answer = \" \".join(context_words[word_start:word_end + 1])\n",
    "                    char_answer = \" \".join(answer_text.strip().split())\n",
    "                    assert char_answer in word_answer\n",
    "\n",
    "                    context_tokens, word_to_token = _tokenize_vocab(vocab, context_words)\n",
    "\n",
    "                    token_start = word_to_token[word_start]\n",
    "                    if word_end < len(word_to_token) - 1:\n",
    "                        token_end = word_to_token[word_end + 1] - 1\n",
    "                    else:\n",
    "                        token_end = len(context_tokens) - 1\n",
    "\n",
    "                    token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, char_answer)\n",
    "\n",
    "                    data = {\"qa_id\": qa_id, \"title\": title, \"question\": vocab.encode_as_pieces(question), \"context\": context_tokens, \"answer\": char_answer, \"token_start\": token_start, \"token_end\":token_end}\n",
    "                    f.write(json.dumps(data, ensure_ascii=False))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "757226d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ca353db2dc4c699d3d31ad9127dfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e39e4bfad364f8cbec17d09a9afa88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 전처리를 수행하여 파일로 생성합니다. \n",
    "dump_korquad(vocab, train_json, f\"{data_dir}/korquad_train.json\")\n",
    "dump_korquad(vocab, dev_json, f\"{data_dir}/korquad_dev.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6225f",
   "metadata": {},
   "source": [
    "- 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6054461c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"qa_id\": \"6566495-0-0\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"를\", \"▁읽고\", \"▁무엇을\", \"▁쓰고\", \"자\", \"▁\", \"했\", \"는\", \"가\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"교향곡\", \"token_start\": 19, \"token_end\": 19}\n",
      "{\"qa_id\": \"6566495-0-1\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"는\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁어디\", \"까지\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했\", \"는\", \"가\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"1악장\", \"token_start\": 168, \"token_end\": 169}\n",
      "{\"qa_id\": \"6566495-0-2\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"가\", \"▁\", \"파우스트\", \"▁서\", \"곡을\", \"▁쓸\", \"▁때\", \"▁어떤\", \"▁곡\", \"의\", \"▁영향을\", \"▁받았\", \"는\", \"가\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"베토벤의 교향곡 9번\", \"token_start\": 80, \"token_end\": 84}\n",
      "{\"qa_id\": \"6566518-0-0\", \"title\": \"파우스트_서곡\", \"question\": [\"▁1839\", \"년\", \"▁바그너\", \"가\", \"▁교향곡\", \"의\", \"▁소재로\", \"▁쓰\", \"려고\", \"▁했던\", \"▁책은\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"파우스트\", \"token_start\": 6, \"token_end\": 7}\n",
      "{\"qa_id\": \"6566518-0-1\", \"title\": \"파우스트_서곡\", \"question\": [\"▁\", \"파우스트\", \"▁서\", \"곡\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"이\", \"▁영향을\", \"▁받은\", \"▁베토벤\", \"의\", \"▁곡은\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"합창교향곡\", \"token_start\": 143, \"token_end\": 146}\n",
      "{\"qa_id\": \"5917067-0-0\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"가\", \"▁\", \"파우스트\", \"를\", \"▁처음으로\", \"▁읽\", \"은\", \"▁\", \"년\", \"도\", \"는\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"1839\", \"token_start\": 0, \"token_end\": 0}\n",
      "{\"qa_id\": \"5917067-0-1\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"가\", \"▁처음\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁한\", \"▁장소\", \"는\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"파리\", \"token_start\": 165, \"token_end\": 165}\n",
      "{\"qa_id\": \"5917067-0-2\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"의\", \"▁1\", \"악장\", \"의\", \"▁초연\", \"은\", \"▁어디서\", \"▁연주\", \"되었\", \"는\", \"가\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"드레스덴\", \"token_start\": 216, \"token_end\": 216}\n",
      "{\"qa_id\": \"6566495-1-0\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"의\", \"▁작품을\", \"▁시인\", \"의\", \"▁피로\", \"▁쓰여\", \"졌다\", \"고\", \"▁극찬\", \"한\", \"▁것은\", \"▁누구\", \"인\", \"가\", \"?\"], \"context\": [\"▁한편\", \"▁1840\", \"년부터\", \"▁바그너\", \"와\", \"▁알고\", \"▁지내던\", \"▁리스트\", \"가\", \"▁잊\", \"혀\", \"져\", \"▁있던\", \"▁1\", \"악장\", \"을\", \"▁부활\", \"시켜\", \"▁1852\", \"년에\", \"▁바이마르\", \"에서\", \"▁연주\", \"했다\", \".\", \"▁이것을\", \"▁계기로\", \"▁바그너\", \"도\", \"▁이\", \"▁작품에\", \"▁다시\", \"▁관심을\", \"▁갖게\", \"▁되었고\", \",\", \"▁그\", \"▁해\", \"▁9\", \"월에는\", \"▁총\", \"보\", \"의\", \"▁반환\", \"을\", \"▁요구\", \"하여\", \"▁이를\", \"▁서\", \"곡으로\", \"▁간\", \"추\", \"린\", \"▁다음\", \"▁수정\", \"을\", \"▁했고\", \"▁브\", \"라이트\", \"코프\", \"흐\", \"▁&\", \"▁헤르\", \"텔\", \"▁출판사\", \"에서\", \"▁출판\", \"할\", \"▁개정\", \"판\", \"도\", \"▁준비\", \"했다\", \".\", \"▁1853\", \"년\", \"▁5\", \"월에는\", \"▁리스트\", \"가\", \"▁이\", \"▁작품이\", \"▁수정\", \"되었다\", \"는\", \"▁것을\", \"▁인정\", \"했지만\", \",\", \"▁끝내\", \"▁바그너\", \"의\", \"▁출판\", \"▁계획은\", \"▁무산\", \"되고\", \"▁말았다\", \".\", \"▁이후\", \"▁1855\", \"년에\", \"▁리스트\", \"가\", \"▁자신의\", \"▁작품\", \"▁\", \"파우스트\", \"▁교향곡\", \"을\", \"▁거의\", \"▁완성\", \"하여\", \"▁그\", \"▁사실을\", \"▁바그너\", \"에게\", \"▁알\", \"렸고\", \",\", \"▁바그너\", \"는\", \"▁다시\", \"▁개정된\", \"▁총\", \"보를\", \"▁리스트\", \"에게\", \"▁보내고\", \"▁브\", \"라이트\", \"코프\", \"흐\", \"▁&\", \"▁헤르\", \"텔\", \"▁출판사\", \"에는\", \"▁20\", \"루이\", \"의\", \"▁금\", \"을\", \"▁받고\", \"▁팔았다\", \".\", \"▁또한\", \"▁그의\", \"▁작품을\", \"▁“\", \"하나\", \"하나\", \"의\", \"▁음\", \"표\", \"가\", \"▁시인\", \"의\", \"▁피로\", \"▁쓰여\", \"졌다\", \"”\", \"며\", \"▁극찬\", \"했던\", \"▁한스\", \"▁폰\", \"▁\", \"뷜\", \"로\", \"가\", \"▁그것을\", \"▁피아노\", \"▁독주\", \"용으로\", \"▁편곡\", \"했는데\", \",\", \"▁리스트\", \"는\", \"▁그것을\", \"▁약간\", \"▁변형\", \"되었을\", \"▁뿐\", \"이라고\", \"▁지적했다\", \".\", \"▁이\", \"▁서\", \"곡\", \"의\", \"▁총\", \"보\", \"▁첫\", \"머리\", \"에는\", \"▁\", \"파우스트\", \"▁1\", \"부의\", \"▁내용\", \"▁중\", \"▁한\", \"▁구절\", \"을\", \"▁인용\", \"하고\", \"▁있다\", \".\"], \"answer\": \"한스 폰 뷜로\", \"token_start\": 164, \"token_end\": 168}\n",
      "{\"qa_id\": \"6566495-1-1\", \"title\": \"파우스트_서곡\", \"question\": [\"▁잊\", \"혀\", \"져\", \"▁있는\", \"▁\", \"파우스트\", \"▁서\", \"곡\", \"▁1\", \"악장\", \"을\", \"▁부활\", \"시킨\", \"▁것은\", \"▁누구\", \"인\", \"가\", \"?\"], \"context\": [\"▁한편\", \"▁1840\", \"년부터\", \"▁바그너\", \"와\", \"▁알고\", \"▁지내던\", \"▁리스트\", \"가\", \"▁잊\", \"혀\", \"져\", \"▁있던\", \"▁1\", \"악장\", \"을\", \"▁부활\", \"시켜\", \"▁1852\", \"년에\", \"▁바이마르\", \"에서\", \"▁연주\", \"했다\", \".\", \"▁이것을\", \"▁계기로\", \"▁바그너\", \"도\", \"▁이\", \"▁작품에\", \"▁다시\", \"▁관심을\", \"▁갖게\", \"▁되었고\", \",\", \"▁그\", \"▁해\", \"▁9\", \"월에는\", \"▁총\", \"보\", \"의\", \"▁반환\", \"을\", \"▁요구\", \"하여\", \"▁이를\", \"▁서\", \"곡으로\", \"▁간\", \"추\", \"린\", \"▁다음\", \"▁수정\", \"을\", \"▁했고\", \"▁브\", \"라이트\", \"코프\", \"흐\", \"▁&\", \"▁헤르\", \"텔\", \"▁출판사\", \"에서\", \"▁출판\", \"할\", \"▁개정\", \"판\", \"도\", \"▁준비\", \"했다\", \".\", \"▁1853\", \"년\", \"▁5\", \"월에는\", \"▁리스트\", \"가\", \"▁이\", \"▁작품이\", \"▁수정\", \"되었다\", \"는\", \"▁것을\", \"▁인정\", \"했지만\", \",\", \"▁끝내\", \"▁바그너\", \"의\", \"▁출판\", \"▁계획은\", \"▁무산\", \"되고\", \"▁말았다\", \".\", \"▁이후\", \"▁1855\", \"년에\", \"▁리스트\", \"가\", \"▁자신의\", \"▁작품\", \"▁\", \"파우스트\", \"▁교향곡\", \"을\", \"▁거의\", \"▁완성\", \"하여\", \"▁그\", \"▁사실을\", \"▁바그너\", \"에게\", \"▁알\", \"렸고\", \",\", \"▁바그너\", \"는\", \"▁다시\", \"▁개정된\", \"▁총\", \"보를\", \"▁리스트\", \"에게\", \"▁보내고\", \"▁브\", \"라이트\", \"코프\", \"흐\", \"▁&\", \"▁헤르\", \"텔\", \"▁출판사\", \"에는\", \"▁20\", \"루이\", \"의\", \"▁금\", \"을\", \"▁받고\", \"▁팔았다\", \".\", \"▁또한\", \"▁그의\", \"▁작품을\", \"▁“\", \"하나\", \"하나\", \"의\", \"▁음\", \"표\", \"가\", \"▁시인\", \"의\", \"▁피로\", \"▁쓰여\", \"졌다\", \"”\", \"며\", \"▁극찬\", \"했던\", \"▁한스\", \"▁폰\", \"▁\", \"뷜\", \"로\", \"가\", \"▁그것을\", \"▁피아노\", \"▁독주\", \"용으로\", \"▁편곡\", \"했는데\", \",\", \"▁리스트\", \"는\", \"▁그것을\", \"▁약간\", \"▁변형\", \"되었을\", \"▁뿐\", \"이라고\", \"▁지적했다\", \".\", \"▁이\", \"▁서\", \"곡\", \"의\", \"▁총\", \"보\", \"▁첫\", \"머리\", \"에는\", \"▁\", \"파우스트\", \"▁1\", \"부의\", \"▁내용\", \"▁중\", \"▁한\", \"▁구절\", \"을\", \"▁인용\", \"하고\", \"▁있다\", \".\"], \"answer\": \"리스트\", \"token_start\": 7, \"token_end\": 7}\n"
     ]
    }
   ],
   "source": [
    "def print_file(filename, count=10):\n",
    "    \"\"\"\n",
    "    파일 내용 출력\n",
    "    :param filename: 파일 이름\n",
    "    :param count: 출력 라인 수\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if count <= i:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "print_file(f\"{data_dir}/korquad_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fdfdc",
   "metadata": {},
   "source": [
    "## 데이터 분석\n",
    "### Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa2a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 ['▁바그너', '는', '▁괴테', '의', '▁', '파우스트', '를', '▁읽고', '▁무엇을', '▁쓰고', '자', '▁', '했', '는', '가', '?']\n",
      "168 ['▁바그너', '는', '▁교향곡', '▁작곡', '을', '▁어디', '까지', '▁쓴', '▁뒤에', '▁중단', '했', '는', '가', '?']\n",
      "80 ['▁바그너', '가', '▁', '파우스트', '▁서', '곡을', '▁쓸', '▁때', '▁어떤', '▁곡', '의', '▁영향을', '▁받았', '는', '가', '?']\n",
      "6 ['▁1839', '년', '▁바그너', '가', '▁교향곡', '의', '▁소재로', '▁쓰', '려고', '▁했던', '▁책은', '?']\n",
      "143 ['▁', '파우스트', '▁서', '곡', '의', '▁라', '단', '조', '▁조성', '이', '▁영향을', '▁받은', '▁베토벤', '의', '▁곡은', '?']\n",
      "0 ['▁바그너', '가', '▁', '파우스트', '를', '▁처음으로', '▁읽', '은', '▁', '년', '도', '는', '?']\n",
      "165 ['▁바그너', '가', '▁처음', '▁교향곡', '▁작곡', '을', '▁한', '▁장소', '는', '?']\n",
      "216 ['▁바그너', '의', '▁1', '악장', '의', '▁초연', '은', '▁어디서', '▁연주', '되었', '는', '가', '?']\n",
      "164 ['▁바그너', '의', '▁작품을', '▁시인', '의', '▁피로', '▁쓰여', '졌다', '고', '▁극찬', '한', '▁것은', '▁누구', '인', '가', '?']\n",
      "7 ['▁잊', '혀', '져', '▁있는', '▁', '파우스트', '▁서', '곡', '▁1', '악장', '을', '▁부활', '시킨', '▁것은', '▁누구', '인', '가', '?']\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "contexts = []\n",
    "token_starts = []\n",
    "with open(f\"{data_dir}/korquad_train.json\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        questions.append(data[\"question\"])\n",
    "        contexts.append(data[\"context\"])\n",
    "        token_starts.append(data[\"token_start\"])\n",
    "        if i < 10:\n",
    "            print(data[\"token_start\"], data[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9899601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question 길이 최대:      58\n",
      "question 길이 최소:       3\n",
      "question 길이 평균:      15.25\n",
      "question 길이 표준편차:    5.50\n",
      "question 25/100분위:    11.00\n",
      "question 50/100분위:    14.00\n",
      "question 75/100분위:    18.00\n",
      "question IQR:           7.00\n",
      "question MAX/100분위:   28.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# token count\n",
    "train_question_counts = [len(question) for question in questions]\n",
    "\n",
    "# 데이터 길이\n",
    "print(f\"question 길이 최대:    {np.max(train_question_counts):4d}\")\n",
    "print(f\"question 길이 최소:    {np.min(train_question_counts):4d}\")\n",
    "print(f\"question 길이 평균:    {np.mean(train_question_counts):7.2f}\")\n",
    "print(f\"question 길이 표준편차: {np.std(train_question_counts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(train_question_counts, 25)\n",
    "percentile50 = np.percentile(train_question_counts, 50)\n",
    "percentile75 = np.percentile(train_question_counts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"question 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"question 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"question 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"question IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"question MAX/100분위: {percentileMAX:7.2f}\")\n",
    "\n",
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_question_counts, bins=100, range=[0, 100], facecolor='b', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Count of question')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of question')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of question')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21beb8",
   "metadata": {},
   "source": [
    "### context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c8f5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context 길이 최대:    4816\n",
      "context 길이 최소:     108\n",
      "context 길이 평균:     222.84\n",
      "context 길이 표준편차:   97.68\n",
      "context 25/100분위:   169.00\n",
      "context 50/100분위:   199.00\n",
      "context 75/100분위:   248.00\n",
      "context IQR:          79.00\n",
      "context MAX/100분위:  366.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# token count\n",
    "train_context_counts = [len(context) for context in contexts]\n",
    "\n",
    "# 데이터 길이\n",
    "print(f\"context 길이 최대:    {np.max(train_context_counts):4d}\")\n",
    "print(f\"context 길이 최소:    {np.min(train_context_counts):4d}\")\n",
    "print(f\"context 길이 평균:    {np.mean(train_context_counts):7.2f}\")\n",
    "print(f\"context 길이 표준편차: {np.std(train_context_counts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(train_context_counts, 25)\n",
    "percentile50 = np.percentile(train_context_counts, 50)\n",
    "percentile75 = np.percentile(train_context_counts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"context 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"context 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"context 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"context IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"context MAX/100분위: {percentileMAX:7.2f}\")\n",
    "\n",
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_context_counts, bins=900, range=[100, 1000], facecolor='r', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Count of context')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of context')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of context')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd9ba2",
   "metadata": {},
   "source": [
    "### answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2008c3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer 위치 최대:    1124\n",
      "answer 위치 최소:       0\n",
      "answer 위치 평균:      89.01\n",
      "answer 위치 표준편차:   78.21\n",
      "answer 25/100분위:    25.00\n",
      "answer 50/100분위:    74.00\n",
      "answer 75/100분위:   134.00\n",
      "answer IQR:         109.00\n",
      "answer MAX/100분위:  297.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_answer_starts = token_starts\n",
    "\n",
    "# 데이터 길이\n",
    "print(f\"answer 위치 최대:    {np.max(train_answer_starts):4d}\")\n",
    "print(f\"answer 위치 최소:    {np.min(train_answer_starts):4d}\")\n",
    "print(f\"answer 위치 평균:    {np.mean(train_answer_starts):7.2f}\")\n",
    "print(f\"answer 위치 표준편차: {np.std(train_answer_starts):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(train_answer_starts, 25)\n",
    "percentile50 = np.percentile(train_answer_starts, 50)\n",
    "percentile75 = np.percentile(train_answer_starts, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"answer 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"answer 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"answer 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"answer IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"answer MAX/100분위: {percentileMAX:7.2f}\")\n",
    "\n",
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_answer_starts, bins=500, range=[0, 500], facecolor='g', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Count of answer')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of answer')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of answer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be64f4c",
   "metadata": {},
   "source": [
    "## 데이터 로드\n",
    "- 지금까지 만든 데이터를 메모리에 로드하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7dffea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "982e71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = os.path.join(data_dir, \"korquad_train.json\")\n",
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")\n",
    "\n",
    "args = Config({\n",
    "    'max_seq_length': 384,\n",
    "    'max_query_length': 64,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24220bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 데이터셋 파일을 메모리에 로딩하는 함수\n",
    "def load_data(args, filename):\n",
    "    inputs, segments, labels_start, labels_end = [], [], [], []\n",
    "\n",
    "    n_discard = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Loading ...\")):\n",
    "            data = json.loads(line)\n",
    "            token_start = data.get(\"token_start\")\n",
    "            token_end = data.get(\"token_end\")\n",
    "            question = data[\"question\"][:args.max_query_length]\n",
    "            context = data[\"context\"]\n",
    "            answer_tokens = \" \".join(context[token_start:token_end + 1])\n",
    "            context_len = args.max_seq_length - len(question) - 3\n",
    "\n",
    "            if token_end >= context_len:\n",
    "                # 최대 길이내에 token이 들어가지 않은 경우 처리하지 않음\n",
    "                n_discard += 1\n",
    "                continue\n",
    "            context = context[:context_len]\n",
    "            assert len(question) + len(context) <= args.max_seq_length - 3\n",
    "\n",
    "            tokens = ['[CLS]'] + question + ['[SEP]'] + context + ['[SEP]']\n",
    "            ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "            ids += [0] * (args.max_seq_length - len(ids))\n",
    "            inputs.append(ids)\n",
    "            segs = [0] * (len(question) + 2) + [1] * (len(context) + 1)\n",
    "            segs += [0] * (args.max_seq_length - len(segs))\n",
    "            segments.append(segs)\n",
    "            token_start += (len(question) + 2)\n",
    "            labels_start.append(token_start)\n",
    "            token_end += (len(question) + 2)\n",
    "            labels_end.append(token_end)\n",
    "    print(f'n_discard: {n_discard}')\n",
    "\n",
    "    return (np.array(inputs), np.array(segments)), (np.array(labels_start), np.array(labels_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d0adda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b7acdc0705427fb05087d178a543ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading ...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discard: 430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb51cc44de1b4eb5b6f6cccd400aa0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading ...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discard: 78\n"
     ]
    }
   ],
   "source": [
    "# train data load\n",
    "train_inputs, train_labels = load_data(args, train_json)\n",
    "\n",
    "# dev data load\n",
    "dev_inputs, dev_labels = load_data(args, dev_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13bfefdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[    5, 15798,    10, ...,     0,     0,     0],\n",
       "         [    5, 15798,    10, ...,     0,     0,     0],\n",
       "         [    5, 15798,    19, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    5, 21666,    19, ...,     0,     0,     0],\n",
       "         [    5,   964, 16865, ...,     0,     0,     0],\n",
       "         [    5,   365,    15, ...,     0,     0,     0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]])),\n",
       " (array([ 37, 184,  98, ...,  74, 190,  35]),\n",
       "  array([ 37, 185, 102, ...,  75, 191,  44])))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c670e813",
   "metadata": {},
   "source": [
    "## BERT 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b5f092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유틸리티 함수들\n",
    "\n",
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n",
    "\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "\n",
    "\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ba5e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode == \"embedding\" 일 경우 Token Embedding Layer 로 사용되는 layer 클래스입니다. \n",
    "\n",
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shared Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee5857fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad5b5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fec8b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d120924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d71cfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a7f383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, enc_tokens, segments):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_tokens: encoder tokens\n",
    "        :param segments: token segments\n",
    "        :return logits_cls: CLS 결과 logits\n",
    "        :return logits_lm: LM 결과 logits\n",
    "        \"\"\"\n",
    "        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = enc_out\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0435e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4KorQuAD(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(name='BERT4KorQuAD')\n",
    "\n",
    "        self.bert = BERT(config)\n",
    "        self.dense = tf.keras.layers.Dense(2)\n",
    "    \n",
    "    def call(self, enc_tokens, segments):\n",
    "        logits_cls, logits_lm = self.bert(enc_tokens, segments)\n",
    "\n",
    "        hidden = self.dense(logits_lm) # (bs, n_seq, 2)\n",
    "        start_logits, end_logits = tf.split(hidden, 2, axis=-1)  # (bs, n_seq, 1), (bs, n_seq, 1)\n",
    "\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        start_outputs = tf.keras.layers.Softmax(name=\"start\")(start_logits)\n",
    "\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        end_outputs = tf.keras.layers.Softmax(name=\"end\")(end_logits)\n",
    "\n",
    "        return start_outputs, end_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35ac09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config({\"d_model\": 512, \"n_head\": 8, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 6, \"n_seq\": 384, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4687064",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)).shuffle(10000).batch(bert_batch_size)\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices((dev_inputs, dev_labels)).batch(bert_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21596",
   "metadata": {},
   "source": [
    "## 학습에 필요한 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fe69d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, loss_fn, acc_fn, optimizer):\n",
    "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
    "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
    "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
    "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
    "\n",
    "    p_bar = tqdm(dataset)\n",
    "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(p_bar):\n",
    "        with tf.GradientTape() as tape:\n",
    "            start_outputs, end_outputs = model(enc_tokens, segments)\n",
    "\n",
    "            start_loss = loss_fn(start_labels, start_outputs)\n",
    "            end_loss = loss_fn(end_labels, end_outputs)\n",
    "            loss = start_loss + end_loss\n",
    "\n",
    "            start_acc = acc_fn(start_labels, start_outputs)\n",
    "            end_acc = acc_fn(end_labels, end_outputs)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        metric_start_loss(start_loss)\n",
    "        metric_end_loss(end_loss)\n",
    "        metric_start_acc(start_acc)\n",
    "        metric_end_acc(end_acc)\n",
    "        if batch % 10 == 9:\n",
    "            p_bar.set_description(f'loss: {metric_start_loss.result():0.4f}, {metric_end_loss.result():0.4f}, acc: {metric_start_acc.result():0.4f}, {metric_end_acc.result():0.4f}')\n",
    "    p_bar.close()\n",
    "\n",
    "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3417ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataset, loss_fn, acc_fn):\n",
    "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
    "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
    "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
    "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
    "\n",
    "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(dataset):\n",
    "        start_outputs, end_outputs = model(enc_tokens, segments)\n",
    "\n",
    "        start_loss = loss_fn(start_labels, start_outputs)\n",
    "        end_loss = loss_fn(end_labels, end_outputs)\n",
    "\n",
    "        start_acc = acc_fn(start_labels, start_outputs)\n",
    "        end_acc = acc_fn(end_labels, end_outputs)\n",
    "\n",
    "        metric_start_loss(start_loss)\n",
    "        metric_end_loss(end_loss)\n",
    "        metric_start_acc(start_acc)\n",
    "        metric_end_acc(end_acc)\n",
    "\n",
    "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd57326",
   "metadata": {},
   "source": [
    "## Pretrained model 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1347a9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT4KorQuAD\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (BERT)                  multiple                  29202944  \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             multiple                  1026      \n",
      "=================================================================\n",
      "Total params: 29,203,970\n",
      "Trainable params: 29,203,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = os.path.join(model_dir, 'bert_pretrain_32000.hdf5')\n",
    "\n",
    "pre_model = BERT4KorQuAD(config)\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    #  pretrained model 을 로드하기 위해 먼저 모델이 생성되어 있어야 한다.\n",
    "    enc_tokens = np.random.randint(0, len(vocab), (4, 10))\n",
    "    segments = np.random.randint(0, 2, (4, 10))\n",
    "    pre_model(enc_tokens, segments)\n",
    "    \n",
    "    # checkpoint 파일로부터 필요한 layer를 불러온다. \n",
    "    pre_model.load_weights(os.path.join(model_dir, \"bert_pretrain_32000.hdf5\"), by_name=True)\n",
    "\n",
    "    pre_model.summary()\n",
    "else:\n",
    "    print('NO Pretrained Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8409a9",
   "metadata": {},
   "source": [
    "## Pretrain 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c8956d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c524588b90674a64b1808a110cec3b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 0 >> loss: 1.7837, 1.9793, acc: 0.5330, 0.5125\n",
      "save best pre_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628621e0a7a54147a907bb63cf94db58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 1 >> loss: 1.7345, 1.9645, acc: 0.5479, 0.5147\n",
      "save best pre_model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513e7d8791334bcd98c8c806e3c20407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 2 >> loss: 1.7659, 2.0056, acc: 0.5706, 0.5311\n",
      "save best pre_model\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "acc_fn = tf.keras.metrics.sparse_categorical_accuracy\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "best_acc = .0\n",
    "patience = 0\n",
    "for epoch in range(3):\n",
    "    train_epoch(pre_model, train_dataset, loss_fn, acc_fn, optimizer)\n",
    "    start_loss, end_loss, start_acc, end_acc = eval_epoch(pre_model, dev_dataset, loss_fn, acc_fn)\n",
    "    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')\n",
    "    acc = start_acc + end_acc\n",
    "    if best_acc < acc:\n",
    "        patience = 0\n",
    "        best_acc = acc\n",
    "        pre_model.save_weights(os.path.join(data_dir, \"korquad_bert_pretrain.hdf5\"))\n",
    "        print(f'save best pre_model')\n",
    "    else:\n",
    "        patience += 1\n",
    "    if 2 <= patience:\n",
    "        print(f'early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa84b1",
   "metadata": {},
   "source": [
    "## Pretrain inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12a6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict(model, question, context):\n",
    "    \"\"\"\n",
    "    입력에 대한 답변 생성하는 함수\n",
    "    :param model: model\n",
    "    :param question: 입력 문자열\n",
    "    :param context: 입력 문자열\n",
    "    \"\"\"\n",
    "    q_tokens = vocab.encode_as_pieces(question)[:args.max_query_length]\n",
    "    c_tokens = vocab.encode_as_pieces(context)[:args.max_seq_length - len(q_tokens) - 3]\n",
    "    tokens = ['[CLS]'] + q_tokens + ['[SEP]'] + c_tokens + ['[SEP]']\n",
    "    token_ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "    segments = [0] * (len(q_tokens) + 2) + [1] * (len(c_tokens) + 1)\n",
    "\n",
    "    y_start, y_end = model(np.array([token_ids]), np.array([segments]))\n",
    "    y_start_idx = K.argmax(y_start, axis=-1)[0].numpy()\n",
    "    y_end_idx = K.argmax(y_end, axis=-1)[0].numpy()\n",
    "    answer_tokens = tokens[y_start_idx:y_end_idx + 1]\n",
    "\n",
    "    return vocab.decode_pieces(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75156302",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")\n",
    "\n",
    "with open(dev_json) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        question = vocab.decode_pieces(data['question'])\n",
    "        context = vocab.decode_pieces(data['context'])\n",
    "        answer = data['answer']\n",
    "        answer_predict = do_predict(pre_model, question, context)\n",
    "        if answer in answer_predict:\n",
    "            print(i)\n",
    "            print(\"질문 : \", question)\n",
    "            print(\"지문 : \", context)\n",
    "            print(\"정답 : \", answer)\n",
    "            print(\"예측 : \", answer_predict, \"\\n\")\n",
    "        if 100 < i:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee8feb",
   "metadata": {},
   "source": [
    "## non_pretrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4639e37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00d80da909141e99d2f2be89f0f2c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 0 >> loss: 5.9506, 5.9506, acc: 0.0023, 0.0028\n",
      "save best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa7fbcfcfd1491eaee24184ce30e38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 1 >> loss: 5.9506, 5.9506, acc: 0.0042, 0.0021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f817ca2be7fa4793914d0a3ca666b6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 2 >> loss: 5.9506, 5.9506, acc: 0.0026, 0.0033\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "acc_fn = tf.keras.metrics.sparse_categorical_accuracy\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "non_best_acc = .0\n",
    "non_patience = 0\n",
    "for epoch in range(3):\n",
    "    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)\n",
    "    non_start_loss, non_end_loss, non_start_acc, non_end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)\n",
    "    print(f'eval {epoch} >> loss: {non_start_loss:0.4f}, {non_end_loss:0.4f}, acc: {non_start_acc:0.4f}, {non_end_acc:0.4f}')\n",
    "    non_acc = non_start_acc + non_end_acc\n",
    "    if non_best_acc < acc:\n",
    "        non_patience = 0\n",
    "        non_best_acc = acc\n",
    "        model.save_weights(os.path.join(data_dir, \"korquad_bert_non_pretrain.hdf5\"))\n",
    "        print(f'save best model')\n",
    "    else:\n",
    "        patience += 1\n",
    "    if 2 <= patience:\n",
    "        print(f'early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ce174",
   "metadata": {},
   "source": [
    "## non-pretrain inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")\n",
    "\n",
    "with open(dev_json) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        question = vocab.decode_pieces(data['question'])\n",
    "        context = vocab.decode_pieces(data['context'])\n",
    "        answer = data['answer']\n",
    "        answer_predict = do_predict(model, question, context)\n",
    "        if answer in answer_predict:\n",
    "            print(i)\n",
    "            print(\"질문 : \", question)\n",
    "            print(\"지문 : \", context)\n",
    "            print(\"정답 : \", answer)\n",
    "            print(\"예측 : \", answer_predict, \"\\n\")\n",
    "        if 100 < i:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a239b",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0134a2",
   "metadata": {},
   "source": [
    "- 이번 과제는 transformer의 연장선상에 있는 BERT 모델이였다. 자연어 처리 분야(NLP)에서 두각을 나타내고 있는 모델이니 만큼 NLP 분야로 나아가게 된다면 제대로 공부해봐야 겠다는 생각이 들었다.\n",
    "- 이전에도 느꼇던 바대로 점점 keras에서 제공해주는 것만으로는 원하는 인사이트를 얻는것이 힘들어 지는듯하다. 좀 있을 방학시즌때 GradientTape을 활용한 모델 설계 부분을 중점적으로 다시 봐야될듯하다.\n",
    "- pretrain 학습 유무에 따라서 학습 결과물이 확연한 차이가 나는걸 볼수 있었다. \n",
    "  - loss를 봤을때 non-pretrain은 5.7부터 시작하는것을 볼수가 있었고, pretrain은 2.3부터 시작하는 것을 볼수가 있었따.\n",
    "  - 두번째로 acc는 non-pretrain 모델은 상승폭이 거의 없다 싶이 했지만, pretrain 모델은 acc 가 유의미하게 상승했던것을 볼수가 있다.\n",
    "  - 이를 통해서 pretrain 모델을 task에 맞게 finetune 하는 방법을 좀더 알아야 겠다는 생각이 들었다.\n",
    " \n",
    " \n",
    "- 모델을 학습하고 나서 보니 결과 값들을 시각화 한다는것을 깜박했었다. 학습이 오래 걸려서 다시 해보지는 못했지만, 다음과 같이 생각해 볼수 있었다.\n",
    "  - train_epoch, eval_epoch의 결과값을 epoch마다 train_history, val_history리스트에 저장을 해둔다.\n",
    "  - 그런다음 pltl.subplot 을 활용해서 시각화를 해면 될듯하다.\n",
    "  \n",
    "- 이번 과제를 하다보니 tensorflow의 메모리 초과 문제가 발생을 했었는데, 이는 데이터의 사이즈를 줄이거나, 성능에 조금 손해를 보더라도 배치 사이즈를 늘리는 방법이 있었다. \n",
    "하지만 조금더 찾아보니 tensorflow의 메모리 크기를 늘려서 사용하는 방법도 존재 했었다.\n",
    "현재 단계에서는 메모리를 늘려서 해결 했지만, 다음번에는 데이터를 가공해 좀더 적은 메모리로도 동작이 되게 해보고 싶은 마음이 들었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
